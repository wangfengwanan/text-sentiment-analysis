{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 1 fields in line 5, saw 2\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-1c21345700f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../senta_data/new_train.tsv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../senta_data/dev.tsv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../senta_data/test.tsv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1034\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'skipfooter not supported for iteration'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1035\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1036\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1037\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 5, saw 2\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('../senta_data/new_train.tsv', sep='\\t')\n",
    "val = pd.read_csv('../senta_data/dev.tsv')\n",
    "test = pd.read_csv('../senta_data/test.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1、构造词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ''.join(train.review.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_counter = Counter(words_list)\n",
    "words_counter['UNK'] = 10000000\n",
    "words_counter['PAD'] = 10000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_counter_list = sorted(words_counter.items(), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2、构建id2word和word2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = {i: j[0] for i, j in enumerate(words_counter_list)}\n",
    "word2id = {j[0]: i for i, j in enumerate(words_counter_list)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3、语料转换成id向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ids_features(sentence):\n",
    "    words = jieba.lcut(sentence)\n",
    "    ids_feature = [word2id.get(word,word2id['UNK']) for word in words]\n",
    "    return ids_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4、添加位置特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = [',', '.', '，', '。', '!', '?', '？', '！', ':', '：', ';', '；']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_position_features(sentence):\n",
    "    words = jieba.lcut(sentence)\n",
    "    position_feature = []\n",
    "    i = 0\n",
    "    for word in words:\n",
    "        i += 1\n",
    "        if word not in punctuation:\n",
    "            position_feature.append(i)\n",
    "        if word in punctuation:\n",
    "            position_feature.append(i)\n",
    "            i = 0\n",
    "    return position_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(df):\n",
    "    data = []\n",
    "    for index, sentence in enumerate(df.review.values):\n",
    "        ids_feature = generate_ids_features(sentence)\n",
    "        position_feature = generate_position_features(sentence)\n",
    "        label = df.label.values[index]\n",
    "        data.append([ids_feature, position_feature, label])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = get_data(train)\n",
    "val_data = get_data(val)\n",
    "test_data = get_data(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5、数据集封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchManager(object):\n",
    "\n",
    "    def __init__(self, data,  batch_size):\n",
    "        self.batch_data = self.sort_and_pad(data, batch_size)\n",
    "        self.len_data = len(self.batch_data)\n",
    "        self.batch_size = batch_size\n",
    "        self._indicator = 0\n",
    "\n",
    "    def sort_and_pad(self, data, batch_size):\n",
    "        num_batch = int(math.ceil(len(data) / batch_size))\n",
    "        sorted_data = sorted(data, key=lambda x: len(x[0]))\n",
    "        batch_data = list()\n",
    "        for i in range(num_batch):\n",
    "            batch_data.append(self.pad_data(sorted_data[i*int(batch_size) : (i+1)*int(batch_size)]))\n",
    "        return batch_data\n",
    "\n",
    "    @staticmethod\n",
    "    def pad_data(data):\n",
    "        ids_features = []\n",
    "        position_features = []\n",
    "        targets = []\n",
    "        max_length = 1277\n",
    "#         max_length = max([len(sentence[0]) for sentence in data])\n",
    "        for line in data:\n",
    "            ids_feature, position_feature, target = line\n",
    "            padding = [0] * (max_length - len(ids_feature))\n",
    "            ids_features.append(ids_feature + padding)\n",
    "            position_features.append(position_feature + padding)\n",
    "            targets.append(target)\n",
    "        return [ids_features, position_features, targets]\n",
    "    \n",
    "    def next_batch(self):\n",
    "        end_indicator = self._indicator + 1\n",
    "        if end_indicator > self.len_data:\n",
    "            self._indicator = 0\n",
    "            end_indicator = 1\n",
    "        if end_indicator > self.len_data:\n",
    "            raise Execption(\"batch_size: %d is too large\" % batch_size)\n",
    "        \n",
    "        batch_data = self.batch_data[self._indicator: end_indicator]\n",
    "        self._indicator = end_indicator\n",
    "        return batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_manager = BatchManager(train_data, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = len(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('word_embedding', reuse=tf.AUTO_REUSE):\n",
    "    word_embedding_vec = tf.get_variable(name='word_embedding_vec', shape=[num_words, 100], initializer=tf.contrib.layers.xavier_initializer())\n",
    "with tf.variable_scope('position_embedding', reuse=tf.AUTO_REUSE):\n",
    "    position_embedding_vec = tf.get_variable(name='seg_embedding_vec', shape=[num_words, 20], initializer=tf.contrib.layers.xavier_initializer())\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    word_weights = sess.run(word_embedding_vec.read_value())\n",
    "    position_weights = sess.run(position_embedding_vec.read_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6、构建模型图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Model(object):\n",
    "#     def __init__(self, num_words, lr, length):\n",
    "#         self.ids_inputs = tf.placeholder(dtype=tf.int64, shape=[None, length], name='ids_inputs')\n",
    "#         self.positions_inputs = tf.placeholder(dtype=tf.int64, shape=[None, length], name='positions_inputs')\n",
    "#         self.target_outputs = tf.placeholder(dtype=tf.int64, shape=[None, 1], name='target_outputs')\n",
    "#         self.keep_prob = tf.placeholder(dtype=tf.float32, name='keep_prob')\n",
    "#         self.num_words = num_words\n",
    "#         self.lr = lr\n",
    "#         self.layers = [{'dilation':1}, {'dilation':1}, {'dilation':2}]\n",
    "        \n",
    "#     def main(self):\n",
    "#         embedding_matrix = self.embedding_layers(self.ids_inputs, self.positions_inputs)\n",
    "#         model_inputs = tf.nn.dropout(embedding_matrix, self.keep_prob)\n",
    "#         model_outputs = self.IDCNN_layer(model_inputs)\n",
    "#         logits = self.project_layer_idcnn(model_outputs)\n",
    "#         loss = self.loss_layer(logits)\n",
    "#         opt = tf.train.AdamOptimizer(self.lr)\n",
    "#         grads_vars = opt.compute_gradients(loss)\n",
    "#         clip_gradients = [(tf.clip_by_value(t=grad, clip_value_max=5, clip_value_min=-5),var)\n",
    "#                       for grad, var in grads_vars if grad is not None]\n",
    "#         train_op = opt.apply_gradients(clip_gradients)\n",
    "#         return loss, logits\n",
    "        \n",
    "#     def embedding_layers(self, ids_inputs, positions_inputs):\n",
    "#         embedding_ids_matrix = tf.nn.embedding_lookup(word_weights, ids_inputs)\n",
    "#         embedding_positions_matrix = tf.nn.embedding_lookup(position_weights, positions_inputs)\n",
    "#         embedding_matrix = tf.concat([embedding_ids_matrix, embedding_positions_matrix], axis=-1)\n",
    "#         return embedding_matrix\n",
    "        \n",
    "#     def IDCNN_layer(self, model_inputs):\n",
    "#         model_inputs = tf.expand_dims(model_inputs, 1)\n",
    "#         with tf.variable_scope(\"idcnn\", reuse=tf.AUTO_REUSE):\n",
    "#             layerInput = tf.layers.conv2d(inputs=model_inputs,\n",
    "#                                         filters=100,\n",
    "#                                         kernel_size=(1, 3),\n",
    "#                                         padding='same',\n",
    "#                                         activation=tf.nn.relu,\n",
    "#                                         name='conv'\n",
    "#                                        )\n",
    "#             print('layerInput',layerInput)\n",
    "#             finalOutFromLayers = []\n",
    "#             totalWidthForLastDim = 0\n",
    "#             for j in range(4):\n",
    "#                 for i in range(len(self.layers)):\n",
    "#                     dilation = self.layers[i]['dilation']\n",
    "#                     isLast = True if i == (len(self.layers) - 1) else False\n",
    "#                     with tf.variable_scope(\"atrous-conv-layer-%d\" % i,reuse=tf.AUTO_REUSE):\n",
    "#                         conv = tf.layers.conv2d(inputs=layerInput,\n",
    "#                                                     filters=100,\n",
    "#                                                     kernel_size=(1, 3),\n",
    "#                                                     padding='same',\n",
    "#                                                     activation=tf.nn.relu,\n",
    "#                                                     dilation_rate=dilation,\n",
    "#                                                     name='conv_%d_%d'%(j,i))\n",
    "#                         print('atrous_conv_%d'%i,conv)\n",
    "#                         if isLast:\n",
    "#                             finalOutFromLayers.append(conv)\n",
    "#                             totalWidthForLastDim += 100\n",
    "#                         layerInput = conv\n",
    "#             finalOut = tf.concat(axis=3, values=finalOutFromLayers)\n",
    "#             finalOut = tf.nn.dropout(finalOut, self.keep_prob)\n",
    "#             print(finalOut)\n",
    "#             finalOut = tf.squeeze(finalOut, [1])\n",
    "#             print(finalOut)\n",
    "#             print(totalWidthForLastDim)\n",
    "#         return finalOut\n",
    "        \n",
    "#     def project_layer_idcnn(self, model_outputs):\n",
    "#         flatten = tf.layers.flatten(model_outputs)\n",
    "#         logits = tf.layers.dense(flatten, 1)\n",
    "#         return logits\n",
    "        \n",
    "#     def loss_layer(self, logits):\n",
    "#         loss = tf.losses.sigmoid_cross_entropy(multi_class_labels=self.target_outputs, logits=logits)\n",
    "# #             loss = tf.losses.sparse_softmax_cross_entropy(labels=target_outputs, logits=pred)\n",
    "#         return loss\n",
    "        \n",
    "#     def create_feed_dict(self, is_train, batch):\n",
    "#         ids, positions, targets = batch[0]\n",
    "#         feed_dict = {\n",
    "#                 self.ids_inputs: np.asarray(ids),\n",
    "#                 self.positions_inputs: np.asarray(positions),\n",
    "#                 self.keep_prob: 1.0,\n",
    "#             }\n",
    "#         if is_train:\n",
    "#             feed_dict[self.target_outputs] = np.asarray(targets).reshape(-1,1)\n",
    "#             feed_dict[self.keep_prob] = 0.5\n",
    "#         return feed_dict\n",
    "    \n",
    "#     def run_step(self, sess, is_train, batch):\n",
    "#         feed_dict = self.create_feed_dict(is_train, batch)\n",
    "#         loss, logits = self.main()\n",
    "#         if is_train:\n",
    "#             loss = sess.run(loss, feed_dict)\n",
    "#             return loss\n",
    "#         else:\n",
    "#             logits = sess.run(logits, feed_dict)\n",
    "#             return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.Session() as sess:\n",
    "#     for i in range(100):\n",
    "#         batch_data = train_manager.next_batch()\n",
    "#         length = len(batch_data[0][0][0])\n",
    "#         model = Model(num_words, 0.0003, length)\n",
    "#         print(model.ids_inputs) \n",
    "#         sess.run(tf.global_variables_initializer())\n",
    "#         loss = model.run_step(sess, True, batch_data)\n",
    "#         print(loss)\n",
    "# #     for i in range(100):\n",
    "# #         batch_data = train_manager.next_batch()\n",
    "# #         feed_dict = model.create_feed_dict(True, batch_data)\n",
    "# #         loss = model.run_step(sess, True, batch_data, feed_dict)\n",
    "# #         print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layerInput Tensor(\"idcnn/conv/Relu:0\", shape=(?, 1, 1277, 100), dtype=float32)\n",
      "atrous_conv_0 Tensor(\"idcnn/atrous-conv-layer-0/conv_0_0/Relu:0\", shape=(?, 1, 1277, 100), dtype=float32)\n",
      "atrous_conv_1 Tensor(\"idcnn/atrous-conv-layer-1/conv_0_1/Relu:0\", shape=(?, 1, 1277, 100), dtype=float32)\n",
      "atrous_conv_2 Tensor(\"idcnn/atrous-conv-layer-2/conv_0_2/Relu:0\", shape=(?, 1, 1277, 100), dtype=float32)\n",
      "atrous_conv_0 Tensor(\"idcnn/atrous-conv-layer-0_1/conv_1_0/Relu:0\", shape=(?, 1, 1277, 100), dtype=float32)\n",
      "atrous_conv_1 Tensor(\"idcnn/atrous-conv-layer-1_1/conv_1_1/Relu:0\", shape=(?, 1, 1277, 100), dtype=float32)\n",
      "atrous_conv_2 Tensor(\"idcnn/atrous-conv-layer-2_1/conv_1_2/Relu:0\", shape=(?, 1, 1277, 100), dtype=float32)\n",
      "atrous_conv_0 Tensor(\"idcnn/atrous-conv-layer-0_2/conv_2_0/Relu:0\", shape=(?, 1, 1277, 100), dtype=float32)\n",
      "atrous_conv_1 Tensor(\"idcnn/atrous-conv-layer-1_2/conv_2_1/Relu:0\", shape=(?, 1, 1277, 100), dtype=float32)\n",
      "atrous_conv_2 Tensor(\"idcnn/atrous-conv-layer-2_2/conv_2_2/Relu:0\", shape=(?, 1, 1277, 100), dtype=float32)\n",
      "atrous_conv_0 Tensor(\"idcnn/atrous-conv-layer-0_3/conv_3_0/Relu:0\", shape=(?, 1, 1277, 100), dtype=float32)\n",
      "atrous_conv_1 Tensor(\"idcnn/atrous-conv-layer-1_3/conv_3_1/Relu:0\", shape=(?, 1, 1277, 100), dtype=float32)\n",
      "atrous_conv_2 Tensor(\"idcnn/atrous-conv-layer-2_3/conv_3_2/Relu:0\", shape=(?, 1, 1277, 100), dtype=float32)\n",
      "Tensor(\"idcnn/dropout/mul:0\", shape=(?, 1, 1277, 400), dtype=float32)\n",
      "Tensor(\"idcnn/Squeeze:0\", shape=(?, 1277, 400), dtype=float32)\n",
      "400\n",
      "Tensor(\"project/flatten/Reshape:0\", shape=(?, 510800), dtype=float32)\n",
      "Tensor(\"project/dense/BiasAdd:0\", shape=(?, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def creat_model():\n",
    "    ids_inputs = tf.placeholder(dtype=tf.int32, shape=[None, 1277])\n",
    "    positions_inputs = tf.placeholder(dtype=tf.int32, shape=[None, 1277])\n",
    "    target_outputs = tf.placeholder(dtype=tf.int32, shape=[None,])\n",
    "    keep_prob = tf.placeholder(dtype=tf.float32)\n",
    "    embedding_ids_matrix = tf.nn.embedding_lookup(word_weights, ids_inputs)\n",
    "    embedding_positions_matrix = tf.nn.embedding_lookup(position_weights, positions_inputs)\n",
    "    embedding_matrix = tf.concat([embedding_ids_matrix, embedding_positions_matrix], axis=-1)\n",
    "    layers = [{'dilation':1}, {'dilation':1}, {'dilation':2}]\n",
    "    model_inputs = tf.nn.dropout(embedding_matrix, keep_prob=keep_prob)\n",
    "    model_inputs = tf.expand_dims(model_inputs, 1)\n",
    "    \n",
    "    with tf.variable_scope(\"idcnn\", reuse=tf.AUTO_REUSE):\n",
    "        layerInput = tf.layers.conv2d(inputs=model_inputs,\n",
    "                                filters=100,\n",
    "                                kernel_size=(1, 3),\n",
    "                                padding='same',\n",
    "                                activation=tf.nn.relu,\n",
    "                                name='conv'\n",
    "                               )\n",
    "        print('layerInput',layerInput)\n",
    "        finalOutFromLayers = []\n",
    "        totalWidthForLastDim = 0\n",
    "        for j in range(4):\n",
    "            for i in range(len(layers)):\n",
    "                dilation = layers[i]['dilation']\n",
    "                isLast = True if i == (len(layers) - 1) else False\n",
    "                with tf.variable_scope(\"atrous-conv-layer-%d\" % i,reuse=tf.AUTO_REUSE):\n",
    "                    conv = tf.layers.conv2d(inputs=layerInput,\n",
    "                                            filters=100,\n",
    "                                            kernel_size=(1, 3),\n",
    "                                            padding='same',\n",
    "                                            activation=tf.nn.relu,\n",
    "                                            dilation_rate=dilation,\n",
    "                                            name='conv_%d_%d'%(j,i))\n",
    "                    print('atrous_conv_%d'%i,conv)\n",
    "                    if isLast:\n",
    "                        finalOutFromLayers.append(conv)\n",
    "                        totalWidthForLastDim += 100\n",
    "                    layerInput = conv\n",
    "        finalOut = tf.concat(axis=3, values=finalOutFromLayers)\n",
    "        finalOut = tf.nn.dropout(finalOut, keep_prob)\n",
    "        print(finalOut)\n",
    "            #Removes dimensions of size 1 from the shape of a tensor. \n",
    "                #从tensor中删除所有大小是1的维度\n",
    "            \n",
    "                #Given a tensor input, this operation returns a tensor of the same type with all dimensions of size 1 removed. If you don’t want to remove all size 1 dimensions, you can remove specific size 1 dimensions by specifying squeeze_dims. \n",
    "            \n",
    "                #给定张量输入，此操作返回相同类型的张量，并删除所有尺寸为1的尺寸。 如果不想删除所有尺寸1尺寸，可以通过指定squeeze_dims来删除特定尺寸1尺寸。\n",
    "        finalOut = tf.squeeze(finalOut, [1])\n",
    "        print(finalOut)\n",
    "        print(totalWidthForLastDim)\n",
    "        \n",
    "    with tf.name_scope('project'):\n",
    "        flatten = tf.layers.flatten(finalOut)\n",
    "        print(flatten)\n",
    "        logits = tf.layers.dense(flatten, 2)\n",
    "        print(logits)\n",
    "        \n",
    "    with tf.name_scope('metrics'):\n",
    "        softmax_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits = logits, labels = target_outputs)\n",
    "        loss = tf.reduce_mean(softmax_loss)\n",
    "        # [0, 1, 5, 4, 2] -> argmax: 2\n",
    "        y_pred = tf.argmax(tf.nn.softmax(logits),\n",
    "                           1, \n",
    "                           output_type = tf.int32)\n",
    "        correct_pred = tf.equal(target_outputs, y_pred)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "#     with tf.variable_scope('loss', reuse=tf.AUTO_REUSE):\n",
    "#         loss = tf.losses.sparse_softmax_cross_entropy(labels=target_outputs, logits=pred)\n",
    "    with tf.name_scope('train_op'):\n",
    "        opt = tf.train.AdamOptimizer(0.003)\n",
    "        grads_vars = opt.compute_gradients(loss)\n",
    "        # with tf.Session() as sess:\n",
    "        #     print(sess.run(grads_vars))\n",
    "        clip_gradients = [(tf.clip_by_value(t=grad, clip_value_max=5, clip_value_min=-5),var)\n",
    "                      for grad, var in grads_vars if grad is not None]\n",
    "        train_op = opt.apply_gradients(clip_gradients)\n",
    "    return ids_inputs,positions_inputs,target_outputs,keep_prob,loss,accuracy,train_op\n",
    "ids_inputs,positions_inputs,target_outputs,keep_prob,loss,accuracy,train_op = creat_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6910472 0.61\n",
      "0.69325656 0.48\n",
      "0.6911047 0.73\n",
      "0.6953199 0.39\n",
      "0.6930731 0.52\n",
      "0.6905945 0.73\n",
      "0.6954097 0.39\n",
      "0.69307417 0.52\n",
      "0.69058967 0.73\n",
      "0.69542384 0.39\n",
      "0.69307554 0.52\n",
      "0.69058985 0.73\n",
      "0.69543093 0.39\n",
      "0.6930763 0.52\n",
      "0.6905899 0.73\n",
      "0.69543487 0.39\n",
      "0.6930769 0.52\n",
      "0.6905899 0.73\n",
      "0.69543755 0.39\n",
      "0.69307864 0.52\n",
      "0.69058937 0.73\n",
      "0.6954394 0.39\n",
      "0.6930774 0.52\n",
      "0.69059014 0.73\n",
      "0.69544053 0.39\n",
      "0.69307756 0.52\n",
      "0.69059014 0.73\n",
      "0.69544137 0.39\n",
      "0.69307756 0.52\n",
      "0.69059014 0.73\n",
      "0.6954419 0.39\n",
      "0.6930777 0.52\n",
      "0.69059014 0.73\n",
      "0.69544244 0.39\n",
      "0.6930777 0.52\n",
      "0.69059014 0.73\n",
      "0.6954427 0.39\n",
      "0.6930778 0.52\n",
      "0.69059014 0.73\n",
      "0.695443 0.39\n",
      "0.69307786 0.52\n",
      "0.69059014 0.73\n",
      "0.6954431 0.39\n",
      "0.6930778 0.52\n",
      "0.69059014 0.73\n",
      "0.6954432 0.39\n",
      "0.6930778 0.52\n",
      "0.69059014 0.73\n",
      "0.6954433 0.39\n",
      "0.69307786 0.52\n",
      "0.69059014 0.73\n",
      "0.6954434 0.39\n",
      "0.69307786 0.52\n",
      "0.69059014 0.73\n",
      "0.6954434 0.39\n",
      "0.69307786 0.52\n",
      "0.69059014 0.73\n",
      "0.6954435 0.39\n",
      "0.69307786 0.52\n",
      "0.69059014 0.73\n",
      "0.6954434 0.39\n",
      "0.69307786 0.52\n",
      "0.69059014 0.73\n",
      "0.6954435 0.39\n",
      "0.6930779 0.52\n",
      "0.69059014 0.73\n",
      "0.6954436 0.39\n",
      "0.69307786 0.52\n",
      "0.69059014 0.73\n",
      "0.6954436 0.39\n",
      "0.69307786 0.52\n",
      "0.69059014 0.73\n",
      "0.6954434 0.39\n",
      "0.69307786 0.52\n",
      "0.5821672 0.73\n",
      "0.6948624 0.39\n",
      "0.69309825 0.52\n",
      "0.690607 0.73\n",
      "0.69540995 0.39\n",
      "0.69307303 0.52\n",
      "0.69058585 0.73\n",
      "0.69541985 0.39\n",
      "0.69307417 0.52\n",
      "0.6905869 0.73\n",
      "0.69542587 0.39\n",
      "0.6930752 0.52\n",
      "0.69058776 0.73\n",
      "0.6954304 0.39\n",
      "0.6930759 0.52\n",
      "0.69058836 0.73\n",
      "0.6954338 0.39\n",
      "0.6930763 0.52\n",
      "0.6905887 0.73\n",
      "0.6954364 0.39\n",
      "0.6930768 0.52\n",
      "0.69058913 0.73\n",
      "0.6954382 0.39\n",
      "0.693077 0.52\n",
      "0.69058937 0.73\n",
      "0.6954397 0.39\n"
     ]
    }
   ],
   "source": [
    "init_op = tf.global_variables_initializer()\n",
    "train_keep_prob_value = 0.5\n",
    "test_keep_prob_value = 1.0\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    for i in range(10000):\n",
    "        batch_data = train_manager.next_batch()\n",
    "        loss_val, acc, _ = sess.run([loss, accuracy, train_op], feed_dict={ids_inputs: batch_data[0][0],\n",
    "                                        positions_inputs: batch_data[0][1],\n",
    "                                        target_outputs: batch_data[0][2],\n",
    "                                        keep_prob:train_keep_prob_value})\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(loss_val, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def IDCNN_layer(model_inputs):\n",
    "#     model_inputs = tf.nn.dropout(model_inputs, keep_prob=0.5)\n",
    "#     model_inputs = tf.expand_dims(model_inputs, 1)\n",
    "#     print(model_inputs)\n",
    "#     with tf.variable_scope(\"idcnn\", reuse=tf.AUTO_REUSE):\n",
    "#         layerInput = tf.layers.conv2d(inputs=model_inputs,\n",
    "#                                 filters=100,\n",
    "#                                 kernel_size=(1, 3),\n",
    "#                                 padding='same',\n",
    "#                                 activation=tf.nn.relu,\n",
    "#                                 name='conv'\n",
    "#                                )\n",
    "#         print('layerInput',layerInput)\n",
    "#         finalOutFromLayers = []\n",
    "#         totalWidthForLastDim = 0\n",
    "#         for j in range(4):\n",
    "#             for i in range(len(layers)):\n",
    "#                 dilation = layers[i]['dilation']\n",
    "#                 isLast = True if i == (len(layers) - 1) else False\n",
    "#                 with tf.variable_scope(\"atrous-conv-layer-%d\" % i,reuse=tf.AUTO_REUSE):\n",
    "#                     conv = tf.layers.conv2d(inputs=layerInput,\n",
    "#                                             filters=100,\n",
    "#                                             kernel_size=(1, 3),\n",
    "#                                             padding='same',\n",
    "#                                             activation=tf.nn.relu,\n",
    "#                                             dilation_rate=dilation,\n",
    "#                                             name='conv_%d_%d'%(j,i))\n",
    "#                     print('atrous_conv_%d'%i,conv)\n",
    "#                     if isLast:\n",
    "#                         finalOutFromLayers.append(conv)\n",
    "#                         totalWidthForLastDim += 100\n",
    "#                     layerInput = conv\n",
    "#         finalOut = tf.concat(axis=3, values=finalOutFromLayers)\n",
    "#         keepProb = 0.5\n",
    "#         finalOut = tf.nn.dropout(finalOut, keepProb)\n",
    "#         print(finalOut)\n",
    "#             #Removes dimensions of size 1 from the shape of a tensor. \n",
    "#                 #从tensor中删除所有大小是1的维度\n",
    "            \n",
    "#                 #Given a tensor input, this operation returns a tensor of the same type with all dimensions of size 1 removed. If you don’t want to remove all size 1 dimensions, you can remove specific size 1 dimensions by specifying squeeze_dims. \n",
    "            \n",
    "#                 #给定张量输入，此操作返回相同类型的张量，并删除所有尺寸为1的尺寸。 如果不想删除所有尺寸1尺寸，可以通过指定squeeze_dims来删除特定尺寸1尺寸。\n",
    "#         finalOut = tf.squeeze(finalOut, [1])\n",
    "#         print(finalOut)\n",
    "#         print(totalWidthForLastDim)\n",
    "#         return finalOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def project_layer_idcnn(idcnn_outputs, name=None):\n",
    "#     with tf.variable_scope('project', reuse=tf.AUTO_REUSE):\n",
    "#         flatten = tf.layers.flatten(idcnn_outputs)\n",
    "#         print(flatten)\n",
    "#         pred = tf.layers.dense(flatten, 2)\n",
    "#         print(pred)\n",
    "#     return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.variable_scope('loss', reuse=tf.AUTO_REUSE):\n",
    "#     loss = tf.losses.sparse_softmax_cross_entropy(labels=target_outputs, logits=project_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt = tf.train.AdamOptimizer(0.0003)\n",
    "# grads_vars = opt.compute_gradients(loss)\n",
    "# # with tf.Session() as sess:\n",
    "# #     print(sess.run(grads_vars))\n",
    "# clip_gradients = [(tf.clip_by_value(t=grad, clip_value_max=5, clip_value_min=-5),var)\n",
    "#                   for grad, var in grads_vars if grad is not None]\n",
    "# train_op = opt.apply_gradients(clip_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     for i in range(100):\n",
    "#         batch_data = train_manager.next_batch()\n",
    "#         _, loss = sess.run([train_op, loss], feed_dict={ids_inputs: batch_data[0][0],\n",
    "#                                                        positions_inputs: batch_data[0][1],\n",
    "#                                                        target_outputs: batch_data[0][2]})\n",
    "#         print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
